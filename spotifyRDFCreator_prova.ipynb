{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "# Load the required libraries\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace\n",
    "\n",
    "# rdflib knows about some namespaces, like FOAF\n",
    "from rdflib.namespace import FOAF, XSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get absolute path\n",
    "absPath = str(Path(os.path.abspath(os.getcwd())).absolute())\n",
    "datasetsPath = os.path.join(absPath, \"datasets\")\n",
    "rdfPath = os.path.join(absPath, \"rdf\")\n",
    "\n",
    "# Create dataset directory if not exists\n",
    "if not os.path.exists(datasetsPath):\n",
    "    os.mkdir(datasetsPath)\n",
    "\n",
    "# Create RDF directory if not exists\n",
    "if not os.path.exists(rdfPath):\n",
    "    os.mkdir(rdfPath)\n",
    "\n",
    "# Setup datasets paths\n",
    "spotifyChartsPath = os.path.join(datasetsPath, \"reducedSpotifyCharts.csv\")\n",
    "genresPath = os.path.join(datasetsPath, \"genres.csv\")\n",
    "marketsPath = os.path.join(datasetsPath, \"markets.csv\")\n",
    "tracksPath = os.path.join(datasetsPath, \"tracks.csv\")\n",
    "albumsPath = os.path.join(datasetsPath, \"albums.csv\")\n",
    "artistsPath = os.path.join(datasetsPath, \"artists.csv\")\n",
    "peoplePath = os.path.join(datasetsPath, \"people.csv\")\n",
    "\n",
    "# Countries\n",
    "countriesPath = os.path.join(datasetsPath, \"countries2.csv\")\n",
    "altCountriesPath = os.path.join(datasetsPath, \"altCountries.csv\")\n",
    "\n",
    "# Setup Turtle paths\n",
    "genresTTLPath = os.path.join(rdfPath, \"genres.ttl\")\n",
    "marketsTTLPath = os.path.join(rdfPath, \"markets.ttl\")\n",
    "tracksTTLPath = os.path.join(rdfPath, \"tracks.ttl\")\n",
    "albumsTTLPath = os.path.join(rdfPath, \"albums.ttl\")\n",
    "artistsTTLPath = os.path.join(rdfPath, \"artists.ttl\")\n",
    "peopleTTLPath = os.path.join(rdfPath, \"people.ttl\")\n",
    "chartsTTLPath = os.path.join(rdfPath, \"charts.ttl\")\n",
    "appearanceTTLPath = os.path.join(rdfPath, \"appearance.ttl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the ontologies namespaces not known by RDFlib\n",
    "\n",
    "# Country Ontology\n",
    "CNS = Namespace(\"http://eulersharp.sourceforge.net/2003/03swap/countries#\")\n",
    "\n",
    "# Spotify Ontology\n",
    "SO = Namespace(\"https://www.dei.unipd.it/~martinelli/spotify/spotifyOntology#\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createGraph():\n",
    "    # Create the graph\n",
    "    g = Graph()\n",
    "\n",
    "    # Bind the namespaces to a prefix for more readable output\n",
    "    g.bind(\"foaf\", FOAF)\n",
    "    g.bind(\"xsd\", XSD)\n",
    "    g.bind(\"countries\", CNS)\n",
    "    g.bind(\"so\", SO)\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadCountries():\n",
    "    countries = pd.read_csv(countriesPath, sep=\",\")\n",
    "    altCountries = pd.read_csv(altCountriesPath, sep=\",\")\n",
    "    altCountries.columns = ['AlternativeName', 'Name']\n",
    "    \n",
    "    return countries, altCountries\n",
    "\n",
    "\n",
    "def getCountryCode(countryName, countries, altCountries):\n",
    "    # Try to retrieve ISO CODE of the country\n",
    "    try:\n",
    "        matchedCountries = countries[countries['Name'].str.contains(countryName)]\n",
    "        countryCode = matchedCountries['Code'].iloc[0]\n",
    "    except IndexError as e:\n",
    "        # Look if an alternative name was used\n",
    "        alternativeMatchedCountries = altCountries[altCountries['AlternativeName'].str.contains(countryName)]\n",
    "        countryName = alternativeMatchedCountries['Name'].iloc[0]\n",
    "\n",
    "        matchedCountries = countries[countries['Name'] == countryName]\n",
    "        countryCode = matchedCountries['Code'].iloc[0]\n",
    "    \n",
    "    return countryCode, countryName\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Graph\n",
    "g = createGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "genres = pd.read_csv(genresPath, sep=\",\", index_col=\"genre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createGenreID(genre):\n",
    "    # Replace all special chars with \"-\"\n",
    "    genreID = \"\"\n",
    "    for char in genre:\n",
    "        genreID += char if char.isalnum() else \"-\"\n",
    "        \n",
    "    return genreID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the album DataFrame\n",
    "for genre, row in genres.iterrows():\n",
    "    # Create genre ID from name\n",
    "    genreID = createGenreID(genre)\n",
    "\n",
    "    # Create the node to add to the Graph\n",
    "    Genre = URIRef(SO[genreID])\n",
    "\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Genre, RDF.type, SO.Genre))\n",
    "\n",
    "    # Add the name of the genre\n",
    "    g.add((Genre, SO[\"name\"], Literal(genre, datatype=XSD.string)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ðŸ’¾] SAVING\n"
     ]
    }
   ],
   "source": [
    "# Save all the data in the Turtle format\n",
    "print(\"[ðŸ’¾] SAVING\")\n",
    "with open(genresTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(g.serialize(format=\"turtle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Graph\n",
    "g = createGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "artists = pd.read_csv(artistsPath, sep=\",\", index_col=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the album DataFrame\n",
    "for artistID, row in artists.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    Artist = URIRef(SO[artistID])\n",
    "\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Artist, RDF.type, SO.Artist))\n",
    "\n",
    "    # Add the name of the artist\n",
    "    g.add((Artist, SO[\"name\"], Literal(row[\"name\"], datatype=XSD.string)))\n",
    "\n",
    "    # Add the popularity of the artist\n",
    "    g.add((Artist, SO[\"popularity\"], Literal(row[\"popularity\"], datatype=XSD.int)))\n",
    "\n",
    "    # Load genres as array\n",
    "    genres = row[\"genres\"].split(\",\") if not pd.isnull(row[\"genres\"]) else []\n",
    "\n",
    "    for genre in genres:\n",
    "        # Create the RDF node\n",
    "        Genre = URIRef(SO[createGenreID(genre)])\n",
    "\n",
    "        # Add the edge connecting the Album and the Country\n",
    "        g.add((Artist, SO[\"hasGenre\"], Genre))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ðŸ’¾] SAVING\n"
     ]
    }
   ],
   "source": [
    "# Save all the data in the Turtle format\n",
    "print(\"[ðŸ’¾] SAVING\")\n",
    "with open(artistsTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(g.serialize(format=\"turtle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Albums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Graph\n",
    "g = createGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "albums = pd.read_csv(albumsPath, sep=\",\", index_col=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the album DataFrame\n",
    "for albumID, row in albums.iterrows():\n",
    "    # Create the node to add to the Graph \n",
    "    Album = URIRef(SO[albumID])\n",
    "\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Album, RDF.type, SO.Album))\n",
    "\n",
    "    # Add the name of the album\n",
    "    g.add((Album, SO[\"name\"], Literal(row[\"title\"], datatype=XSD.string)))\n",
    "    \n",
    "    # Add the total tracks of the album\n",
    "    g.add((Album, SO[\"totalTracks\"], Literal(row[\"total_tracks\"], datatype=XSD.int)))\n",
    "\n",
    "    # Manage release date taking into account release precision\n",
    "    releaseDate = row[\"release_date\"]\n",
    "    if(row[\"release_date_precision\"]==\"year\"):\n",
    "        releaseDate += \"-01-01\"\n",
    "    elif(row[\"release_date_precision\"]==\"month\"):\n",
    "        releaseDate += \"-01\"\n",
    "    \n",
    "    # Add the release date of the album\n",
    "    g.add((Album, SO[\"releaseDate\"], Literal(releaseDate, datatype=XSD.date)))    \n",
    "    \n",
    "    # Add album type\n",
    "    albumType = URIRef(SO[row[\"album_type\"]])\n",
    "    g.add((Album, SO[\"isTypeOf\"], albumType))  \n",
    "\n",
    "    # Load countries as array\n",
    "    countries = row[\"available_countries\"].split(\",\") if not pd.isnull(row[\"available_countries\"]) else []\n",
    "\n",
    "    for country in countries:\n",
    "        # Create the RDF node\n",
    "        Country = URIRef(CNS[country.lower()])\n",
    "\n",
    "        # Add the edge connecting the Album and the Country \n",
    "        g.add((Album, SO[\"isAvailableIn\"], Country))\n",
    "    \n",
    "    # Load artists as array\n",
    "    artists = row[\"artists\"].split(\",\")\n",
    "\n",
    "    for artistID in artists:\n",
    "        # Create the RDF node\n",
    "        Artist = URIRef(SO[artistID])\n",
    "\n",
    "        # Add the edge connecting the Album and the Artist\n",
    "        g.add((Artist, SO[\"partecipateIn\"], Album))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ðŸ’¾] SAVING\n"
     ]
    }
   ],
   "source": [
    "# Save all the data in the Turtle format\n",
    "print(\"[ðŸ’¾] SAVING\")\n",
    "with open(albumsTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(g.serialize(format=\"turtle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Graph\n",
    "g = createGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "tracks = pd.read_csv(tracksPath, sep=\",\", index_col=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the tracks DataFrame\n",
    "\n",
    "for trackID, row in tracks.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    Track = URIRef(SO[trackID])\n",
    "\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Track, RDF.type, SO.Track))\n",
    "\n",
    "    # Add the name of the track\n",
    "    g.add((Track, SO[\"name\"], Literal(row[\"title\"], datatype=XSD.string)))\n",
    "\n",
    "    # Add all the technical charateristics\n",
    "    g.add((Track, SO[\"duration\"], Literal(row[\"duration\"], datatype=XSD.int)))\n",
    "    g.add((Track, SO[\"popularity\"], Literal(row[\"popularity\"], datatype=XSD.int)))\n",
    "    g.add((Track, SO[\"explicit\"], Literal(row[\"explicit\"], datatype=XSD.boolean)))\n",
    "    g.add((Track, SO[\"key\"], Literal(row[\"key\"], datatype=XSD.int)))\n",
    "    g.add((Track, SO[\"tempo\"], Literal(row[\"tempo\"], datatype=XSD.float)))\n",
    "    g.add((Track, SO[\"mode\"], Literal(row[\"mode\"], datatype=XSD.int)))\n",
    "    g.add((Track, SO[\"time_signature\"], Literal(row[\"time_signature\"], datatype=XSD.int)))\n",
    "    g.add((Track, SO[\"acousticness\"], Literal(row[\"acousticness\"], datatype=XSD.float)))\n",
    "    g.add((Track, SO[\"danceability\"], Literal(row[\"danceability\"], datatype=XSD.float)))\n",
    "    g.add((Track, SO[\"energy\"], Literal(row[\"energy\"], datatype=XSD.float)))\n",
    "    g.add((Track, SO[\"loudness\"], Literal(row[\"loudness\"], datatype=XSD.float)))\n",
    "    g.add((Track, SO[\"liveness\"], Literal(row[\"liveness\"], datatype=XSD.float)))\n",
    "    g.add((Track, SO[\"valence\"], Literal(row[\"valence\"], datatype=XSD.float)))\n",
    "    g.add((Track, SO[\"speechiness\"], Literal(row[\"speechiness\"], datatype=XSD.float)))\n",
    "    g.add((Track, SO[\"instrumentalness\"], Literal(row[\"instrumentalness\"], datatype=XSD.float)))\n",
    "\n",
    "    # Load countries as array\n",
    "    countries = row[\"available_countries\"].split(\",\") if not pd.isnull(row[\"available_countries\"]) else []\n",
    "\n",
    "    for country in countries:\n",
    "        # Create the RDF node\n",
    "        Country = URIRef(CNS[country.lower()])\n",
    "\n",
    "        # Add the edge connecting the Track and the Country\n",
    "        g.add((Track, SO[\"isAvailableIn\"], Country))\n",
    "\n",
    "    # Load artists as array\n",
    "    artists = row[\"artists\"].split(\",\")\n",
    "\n",
    "    for artistID in artists:\n",
    "        # Create the RDF node\n",
    "        Artist = URIRef(SO[artistID])\n",
    "\n",
    "        # Add the edge connecting the Track and the Artist\n",
    "        g.add((Artist, SO[\"partecipateIn\"], Track))\n",
    "\n",
    "    #Retrieve albumID\n",
    "    albumID = row[\"album\"]\n",
    "\n",
    "    # Create the RDF node\n",
    "    Album = URIRef(SO[albumID])\n",
    "\n",
    "    # Add the edge connecting the Track and the Artist\n",
    "    g.add((Track, SO[\"isPartOf\"], Album))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ðŸ’¾] SAVING\n"
     ]
    }
   ],
   "source": [
    "# Save all the data in the Turtle format\n",
    "print(\"[ðŸ’¾] SAVING\")\n",
    "with open(tracksTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(g.serialize(format=\"turtle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Graph\n",
    "g = createGraph()\n",
    "\n",
    "# Load countries dataframe\n",
    "countries, altCountries = loadCountries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "charts = pd.read_csv(spotifyChartsPath , sep=\",\")\n",
    "\n",
    "# Aggregate the original dataframe to identify a specific chart using COUNTRY and DATE\n",
    "chartsDF = charts.groupby(['country', 'date']).size().reset_index(name='total_tracks')\n",
    "\n",
    "# Removing global\n",
    "chartsDF = chartsDF.drop(index=chartsDF[chartsDF['country'] == 'Global'].index)\n",
    "charts = charts.drop(index=charts[charts['country'] == 'Global'].index)\n",
    "\n",
    "totalRows = len(charts.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I iterate through the dataframe\n",
    "for index, row in chartsDF.iterrows():\n",
    "\n",
    "    # Retrieve country and date\n",
    "    countryName = row[\"country\"]\n",
    "    date = row[\"date\"]\n",
    "    topNumType = 100\n",
    "    numTotalTracks = row[\"total_tracks\"]\n",
    "\n",
    "    # Reformat date\n",
    "    date = datetime.datetime.strptime(date, '%d/%m/%Y').strftime('%d-%m-%y')\n",
    "\n",
    "    # Get the country code\n",
    "    countryCode, _ = getCountryCode(countryName, countries, altCountries)\n",
    "\n",
    "    # Create a uniqueID\n",
    "    chartID = \"top-{}-{}-{}\".format(topNumType, countryCode, date)\n",
    "\n",
    "    # Create the node to add to the Graph\n",
    "    Chart = URIRef(SO[chartID])\n",
    "\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Chart, RDF.type, SO.Chart))\n",
    "\n",
    "    # Add the name of the Chart\n",
    "    chartName = \"TOP {} {}\".format(topNumType, countryName)\n",
    "    g.add((Chart, SO[\"name\"], Literal(chartName, datatype=XSD.string)))\n",
    "\n",
    "    # Add the date of the chart\n",
    "    g.add((Chart, SO[\"date\"], Literal(date, datatype=XSD.date)))\n",
    "\n",
    "    # Add the number of tracks\n",
    "    g.add((Chart, SO[\"totalTracks\"], Literal(row['total_tracks'], datatype=XSD.int)))\n",
    "\n",
    "    # Add related Country\n",
    "    # Create the RDF node\n",
    "    Country = URIRef(CNS[countryCode.lower()])\n",
    "   \n",
    "    # Add the edge connecting the Chart and the Country\n",
    "    g.add((Chart, SO[\"isReferredTo\"], Country))\n",
    "\n",
    "    # Add chart type\n",
    "    chartType = URIRef(SO[\"top\"])\n",
    "    g.add((Chart, SO[\"isTypeOf\"], chartType))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ðŸ’¾] SAVING\n"
     ]
    }
   ],
   "source": [
    "# Save all the data in the Turtle format\n",
    "print(\"[ðŸ’¾] SAVING\")\n",
    "with open(chartsTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(g.serialize(format=\"turtle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appearance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Graph\n",
    "g = createGraph()\n",
    "\n",
    "# Load countries dataframe\n",
    "countries, altCountries = loadCountries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ [STATUS INFO] 10000/682084 (1.47%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 20000/682084 (2.93%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 30000/682084 (4.40%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 40000/682084 (5.86%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 50000/682084 (7.33%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 60000/682084 (8.80%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 70000/682084 (10.26%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 80000/682084 (11.73%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 90000/682084 (13.19%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 100000/682084 (14.66%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 110000/682084 (16.13%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 120000/682084 (17.59%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 130000/682084 (19.06%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 140000/682084 (20.53%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 150000/682084 (21.99%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 160000/682084 (23.46%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 170000/682084 (24.92%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 180000/682084 (26.39%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 190000/682084 (27.86%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 200000/682084 (29.32%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 210000/682084 (30.79%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 220000/682084 (32.25%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 230000/682084 (33.72%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 240000/682084 (35.19%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 250000/682084 (36.65%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 260000/682084 (38.12%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 270000/682084 (39.58%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 280000/682084 (41.05%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 290000/682084 (42.52%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 300000/682084 (43.98%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 310000/682084 (45.45%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 320000/682084 (46.92%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 330000/682084 (48.38%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 340000/682084 (49.85%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 350000/682084 (51.31%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 360000/682084 (52.78%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 370000/682084 (54.25%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 380000/682084 (55.71%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 390000/682084 (57.18%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 400000/682084 (58.64%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 410000/682084 (60.11%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 420000/682084 (61.58%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 430000/682084 (63.04%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 450000/682084 (65.97%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 460000/682084 (67.44%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 470000/682084 (68.91%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 480000/682084 (70.37%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 490000/682084 (71.84%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 500000/682084 (73.30%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 520000/682084 (76.24%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 530000/682084 (77.70%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 540000/682084 (79.17%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 550000/682084 (80.64%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 560000/682084 (82.10%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 570000/682084 (83.57%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 580000/682084 (85.03%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 590000/682084 (86.50%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 600000/682084 (87.97%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 610000/682084 (89.43%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 620000/682084 (90.90%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 630000/682084 (92.36%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 640000/682084 (93.83%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 650000/682084 (95.30%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 660000/682084 (96.76%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 670000/682084 (98.23%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 680000/682084 (99.69%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 690000/682084 (101.16%)\n",
      "\n",
      "ðŸ’¾ [STATUS INFO] 700000/682084 (102.63%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# I iterate through the dataframe\n",
    "for index, row in charts.iterrows():\n",
    "\n",
    "    # Create a uniqueID\n",
    "    appearanceID = \"appearance-{}\".format(index)\n",
    "\n",
    "    # Create the node to add to the Graph\n",
    "    Appearance = URIRef(SO[appearanceID])\n",
    "\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Appearance, RDF.type, SO.Appearance))\n",
    "\n",
    "    # Add the position of track\n",
    "    g.add((Chart, SO[\"position\"], Literal(row['position'], datatype=XSD.int)))\n",
    "\n",
    "    # Get the track id from the uri\n",
    "    trackID = row['uri'].removeprefix(\"https://open.spotify.com/track/\")\n",
    "\n",
    "    # Add the edge connecting Appearance to the Track\n",
    "    Track = URIRef(SO[trackID])\n",
    "    g.add((Track, SO[\"appearsIn\"], Appearance))\n",
    "\n",
    "    # Retrieve country and date\n",
    "    countryName = row[\"country\"]\n",
    "    date = row[\"date\"]\n",
    "\n",
    "    # Reformat date\n",
    "    date = datetime.datetime.strptime(date, '%d/%m/%Y').strftime('%d-%m-%y')\n",
    "\n",
    "    # Get the country code\n",
    "    countryCode, _ = getCountryCode(countryName, countries, altCountries)\n",
    "\n",
    "    # Create a uniqueID\n",
    "    chartID = \"top-100-{}-{}\".format(countryCode, date)\n",
    "\n",
    "    # Create the node to add to the Graph\n",
    "    Chart = URIRef(SO[chartID])\n",
    "\n",
    "    g.add((Appearance, SO[\"isPositionedIn\"], Chart))\n",
    "\n",
    "    if index % 10000 == 0:\n",
    "        print(\"ðŸ’¾ [STATUS INFO] {row}/{totalRows} ({percentage:.2f}%)\\n\".format(\n",
    "            row=index, totalRows=totalRows, percentage=((index * 100) / totalRows)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ðŸ’¾] SAVING\n"
     ]
    }
   ],
   "source": [
    "# Save all the data in the Turtle format\n",
    "print(\"[ðŸ’¾] SAVING\")\n",
    "with open(appearanceTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(g.serialize(format=\"turtle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## People"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Graph\n",
    "g = createGraph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "people = pd.read_csv(peoplePath, sep=\",\", index_col=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedPeople = people.groupby([\"name\",\"surname\"])\n",
    "\n",
    "counter = 0\n",
    "\n",
    "# iterate over each group\n",
    "for peopleID, peopleGroup in groupedPeople:\n",
    "   peopleObj = {}\n",
    "\n",
    "   peopleObj[\"id\"] = peopleID\n",
    "   peopleObj[\"name\"] = peopleGroup.iloc[0][\"name\"] if not pd.isnull(peopleGroup.iloc[0][\"name\"]) else None\n",
    "   peopleObj[\"surname\"] = peopleGroup.iloc[0][\"surname\"] if not pd.isnull(peopleGroup.iloc[0][\"surname\"]) else None\n",
    "   peopleObj[\"birthdate\"] = peopleGroup.iloc[0][\"birthdate\"] if not pd.isnull(peopleGroup.iloc[0][\"birthdate\"]) else None\n",
    "   peopleObj[\"deathdate\"] = peopleGroup.iloc[0][\"deathdate\"] if not pd.isnull(peopleGroup.iloc[0][\"deathdate\"]) else None\n",
    "   peopleObj[\"nationality\"] = peopleGroup.iloc[0][\"nationality\"] if not pd.isnull(peopleGroup.iloc[0][\"nationality\"]) else None\n",
    "   peopleObj[\"complete_name\"] = peopleGroup.iloc[0][\"complete_name\"] if not pd.isnull(peopleGroup.iloc[0][\"complete_name\"]) else None\n",
    "   peopleObj[\"entity_name\"] = peopleGroup.iloc[0][\"entity_name\"] if not pd.isnull(peopleGroup.iloc[0][\"entity_name\"]) else None\n",
    "\n",
    "   artists = []\n",
    "   for index, row in peopleGroup.iterrows():\n",
    "      artists.append(row[\"artist\"])\n",
    "    \n",
    "   peopleObj[\"artists\"] = artists\n",
    "\n",
    "   #print(json.dumps(peopleObj, indent=2))\n",
    "\n",
    "   # Create a uniqueID\n",
    "   peopleID = \"People-{}\".format(counter)\n",
    "   counter+=1\n",
    "\n",
    "   # Create the node to add to the Graph\n",
    "   People = URIRef(SO[peopleID])\n",
    "   # Add triples using store's add() method.\n",
    "   g.add((People, RDF.type, SO.People))\n",
    "\n",
    "   # Add name and surname\n",
    "   g.add((People, SO[\"name\"], Literal(peopleObj[\"name\"], datatype=XSD.string)))\n",
    "   g.add((People, SO[\"surname\"], Literal(peopleObj[\"surname\"], datatype=XSD.string)))\n",
    "\n",
    "   # Manage dates\n",
    "   birthdate = peopleObj[\"birthdate\"]\n",
    "   if (birthdate is not None):\n",
    "      if(len(birthdate)==4):\n",
    "         birthdate += \"-01-01\"\n",
    "      elif(len(birthdate)==7):\n",
    "         birthdate += \"-01\"\n",
    "\n",
    "   deathdate = peopleObj[\"deathdate\"]\n",
    "   if (deathdate is not None):\n",
    "      if(len(deathdate)==4):\n",
    "         deathdate += \"-01-01\"\n",
    "      elif(len(deathdate)==7):\n",
    "         deathdate += \"-01\"\n",
    "\n",
    "   g.add((People, SO[\"birthdate\"], Literal(birthdate, datatype=XSD.date)))\n",
    "   g.add((People, SO[\"deathdate\"], Literal(deathdate, datatype=XSD.date)))\n",
    "\n",
    "   \n",
    "   # Add nationality\n",
    "   # Create the RDF node\n",
    "   if (peopleObj[\"nationality\"] is not None):\n",
    "      Country = URIRef(CNS[peopleObj[\"nationality\"].lower()])\n",
    "   # Add the edge connecting People and the Country\n",
    "   g.add((People, SO[\"hasNationality\"], Country))\n",
    "\n",
    "   #manage edge connectig artists and people\n",
    "   for artistID in peopleObj[\"artists\"]:\n",
    "      # Create the RDF node\n",
    "      Artist = URIRef(SO[artistID])\n",
    "\n",
    "      # Add the edge connecting the Track and the Artist\n",
    "      g.add((People, SO[\"isMemberOf\"], Artist))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ðŸ’¾] SAVING\n"
     ]
    }
   ],
   "source": [
    "# Save all the data in the Turtle format\n",
    "print(\"[ðŸ’¾] SAVING\")\n",
    "with open(peopleTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(g.serialize(format=\"turtle\"))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "22d38fa0cb949c4d83b127034afdff90d77a7338f5681221558c482c7c131893"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
