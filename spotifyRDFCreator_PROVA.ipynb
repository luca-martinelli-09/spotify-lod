{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the required libraries\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace\n",
    "\n",
    "# rdflib knows about some namespaces, like FOAF\n",
    "from rdflib.namespace import FOAF, XSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get absolute path\n",
    "absPath = str(Path(os.path.abspath(os.getcwd())).absolute())\n",
    "datasetsPath = os.path.join(absPath, \"datasets\")\n",
    "rdfPath = os.path.join(absPath, \"rdf\")\n",
    "\n",
    "# Create dataset directory if not exists\n",
    "if not os.path.exists(datasetsPath):\n",
    "    os.mkdir(datasetsPath)\n",
    "\n",
    "# Create RDF directory if not exists\n",
    "if not os.path.exists(rdfPath):\n",
    "    os.mkdir(rdfPath)\n",
    "\n",
    "# Setup datasets paths\n",
    "spotifyChartsPath = os.path.join(datasetsPath, \"reducedSpotifyCharts.csv\")\n",
    "genresPath = os.path.join(datasetsPath, \"genres.csv\")\n",
    "marketsPath = os.path.join(datasetsPath, \"markets.csv\")\n",
    "tracksPath = os.path.join(datasetsPath, \"tracks.csv\")\n",
    "albumsPath = os.path.join(datasetsPath, \"albums.csv\")\n",
    "artistsPath = os.path.join(datasetsPath, \"artists.csv\")\n",
    "peoplePath = os.path.join(datasetsPath, \"people.csv\")\n",
    "\n",
    "# Setup Turtle paths\n",
    "genresTTLPath = os.path.join(rdfPath, \"genres.ttl\")\n",
    "marketsTTLPath = os.path.join(rdfPath, \"markets.ttl\")\n",
    "tracksTTLPath = os.path.join(rdfPath, \"tracks.ttl\")\n",
    "albumsTTLPath = os.path.join(rdfPath, \"albums.ttl\")\n",
    "artistsTTLPath = os.path.join(rdfPath, \"artists.ttl\")\n",
    "peopleTTLPath = os.path.join(rdfPath, \"people.ttl\")\n",
    "appearanceTTLPath = os.path.join(rdfPath, \"appearance.ttl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the ontologies namespaces not known by RDFlib\n",
    "\n",
    "# Country Ontology\n",
    "CNS = Namespace(\"http://eulersharp.sourceforge.net/2003/03swap/countries#\")\n",
    "\n",
    "# Spotify Ontology\n",
    "SO = Namespace(\"https://www.dei.unipd.it/~martinelli/spotify/spotifyOntology#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createGraph():\n",
    "    # Create the graph\n",
    "    g = Graph()\n",
    "\n",
    "    # Bind the namespaces to a prefix for more readable output\n",
    "    g.bind(\"foaf\", FOAF)\n",
    "    g.bind(\"xsd\", XSD)\n",
    "    g.bind(\"countries\", CNS)\n",
    "    g.bind(\"so\", SO)\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Graph\n",
    "g = createGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "genres = pd.read_csv(genresPath, sep=\",\", index_col=\"genre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createGenreID(genre):\n",
    "    # Replace all special chars with \"-\"\n",
    "    genreID = \"\"\n",
    "    for char in genre:\n",
    "        genreID += char if char.isalnum() else \"-\"\n",
    "        \n",
    "    return genreID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the album DataFrame\n",
    "for genre, row in genres.iterrows():\n",
    "    # Create genre ID from name\n",
    "    genreID = createGenreID(genre)\n",
    "\n",
    "    # Create the node to add to the Graph\n",
    "    Genre = URIRef(SO[genreID])\n",
    "\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Genre, RDF.type, SO.Genre))\n",
    "\n",
    "    # Add the name of the genre\n",
    "    g.add((Genre, SO[\"name\"], Literal(genre, datatype=XSD.string)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ðŸ’¾] SAVING\n"
     ]
    }
   ],
   "source": [
    "# Save all the data in the Turtle format\n",
    "print(\"[ðŸ’¾] SAVING\")\n",
    "with open(genresTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(g.serialize(format=\"turtle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Graph\n",
    "g = createGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "artists = pd.read_csv(artistsPath, sep=\",\", index_col=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the album DataFrame\n",
    "for artistID, row in artists.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    Artist = URIRef(SO[artistID])\n",
    "\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Artist, RDF.type, SO.Artist))\n",
    "\n",
    "    # Add the name of the artist\n",
    "    g.add((Artist, SO[\"name\"], Literal(row[\"name\"], datatype=XSD.string)))\n",
    "\n",
    "    # Add the popularity of the artist\n",
    "    g.add((Artist, SO[\"popularity\"], Literal(row[\"popularity\"], datatype=XSD.int)))\n",
    "\n",
    "    # Load genres as array\n",
    "    genres = row[\"genres\"].split(\",\") if not pd.isnull(row[\"genres\"]) else []\n",
    "\n",
    "    for genre in genres:\n",
    "        # Create the RDF node\n",
    "        Genre = URIRef(SO[createGenreID(genre)])\n",
    "\n",
    "        # Add the edge connecting the Album and the Country\n",
    "        g.add((Artist, SO[\"hasGenre\"], Genre))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ðŸ’¾] SAVING\n"
     ]
    }
   ],
   "source": [
    "# Save all the data in the Turtle format\n",
    "print(\"[ðŸ’¾] SAVING\")\n",
    "with open(artistsTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(g.serialize(format=\"turtle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Albums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Graph\n",
    "g = createGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "albums = pd.read_csv(albumsPath, sep=\",\", index_col=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the album DataFrame\n",
    "for albumID, row in albums.iterrows():\n",
    "    # Create the node to add to the Graph \n",
    "    Album = URIRef(SO[albumID])\n",
    "\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Album, RDF.type, SO.Album))\n",
    "\n",
    "    # Add the name of the album\n",
    "    g.add((Album, SO[\"name\"], Literal(row[\"title\"], datatype=XSD.string)))\n",
    "    \n",
    "    # Add the total tracks of the album\n",
    "    g.add((Album, SO[\"totalTracks\"], Literal(row[\"total_tracks\"], datatype=XSD.int)))\n",
    "\n",
    "    # Manage release date taking into account release precision\n",
    "    releaseDate = row[\"release_date\"]\n",
    "    if(row[\"release_date_precision\"]==\"year\"):\n",
    "        releaseDate += \"-01-01\"\n",
    "    elif(row[\"release_date_precision\"]==\"month\"):\n",
    "        releaseDate += \"-01\"\n",
    "    \n",
    "    # Add the release date of the album\n",
    "    g.add((Album, SO[\"releaseDate\"], Literal(releaseDate, datatype=XSD.date)))    \n",
    "    \n",
    "    # Add album type\n",
    "    albumType = URIRef(SO[row[\"album_type\"]])\n",
    "    g.add((Album, SO[\"isTypeOf\"], albumType))  \n",
    "\n",
    "    # Load countries as array\n",
    "    countries = row[\"available_countries\"].split(\",\") if not pd.isnull(row[\"available_countries\"]) else []\n",
    "\n",
    "    for country in countries:\n",
    "        # Create the RDF node\n",
    "        Country = URIRef(CNS[country.lower()])\n",
    "\n",
    "        # Add the edge connecting the Album and the Country \n",
    "        g.add((Album, SO[\"isAvailableIn\"], Country))  \n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ðŸ’¾] SAVING\n"
     ]
    }
   ],
   "source": [
    "# Save all the data in the Turtle format\n",
    "print(\"[ðŸ’¾] SAVING\")\n",
    "with open(albumsTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(g.serialize(format=\"turtle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Graph\n",
    "g = createGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "tracks = pd.read_csv(tracksPath, sep=\",\", index_col=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the tracks DataFrame\n",
    "\n",
    "for trackID, row in tracks.iterrows():\n",
    "    # Create the node to add to the Graph \n",
    "    Track = URIRef(SO[trackID])\n",
    "\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Track, RDF.type, SO.Track))\n",
    "\n",
    "    # Add the name of the track\n",
    "    g.add((Track, SO[\"name\"], Literal(row[\"title\"], datatype=XSD.string)))\n",
    "    \n",
    "    # Add all the technical charateristics\n",
    "    g.add((Track, SO[\"duration\"], Literal(row[\"duration\"], datatype=XSD.int)))\n",
    "    g.add((Track, SO[\"popularity\"], Literal(row[\"popularity\"], datatype=XSD.int)))\n",
    "    g.add((Track, SO[\"explicit\"], Literal(row[\"explicit\"], datatype=XSD.boolean)))\n",
    "    g.add((Track, SO[\"key\"], Literal(row[\"key\"], datatype=XSD.int)))\n",
    "    g.add((Track, SO[\"tempo\"], Literal(row[\"tempo\"], datatype=XSD.float)))\n",
    "    g.add((Track, SO[\"mode\"], Literal(row[\"mode\"], datatype=XSD.int)))\n",
    "    g.add((Track, SO[\"time_signature\"], Literal(row[\"time_signature\"], datatype=XSD.int)))\n",
    "    g.add((Track, SO[\"acousticness\"], Literal(row[\"acousticness\"], datatype=XSD.float)))\n",
    "    g.add((Track, SO[\"danceability\"], Literal(row[\"danceability\"], datatype=XSD.float)))\n",
    "    g.add((Track, SO[\"energy\"], Literal(row[\"energy\"], datatype=XSD.float)))\n",
    "    g.add((Track, SO[\"loudness\"], Literal(row[\"loudness\"], datatype=XSD.float)))\n",
    "    g.add((Track, SO[\"liveness\"], Literal(row[\"liveness\"], datatype=XSD.float)))\n",
    "    g.add((Track, SO[\"valence\"], Literal(row[\"valence\"], datatype=XSD.float)))\n",
    "    g.add((Track, SO[\"speechiness\"], Literal(row[\"speechiness\"], datatype=XSD.float)))\n",
    "    g.add((Track, SO[\"instrumentalness\"], Literal(row[\"instrumentalness\"], datatype=XSD.float)))\n",
    "\n",
    "    # Load countries as array\n",
    "    countries = row[\"available_countries\"].split(\",\") if not pd.isnull(row[\"available_countries\"]) else []\n",
    "\n",
    "    for country in countries:\n",
    "        # Create the RDF node\n",
    "        Country = URIRef(CNS[country.lower()])\n",
    "\n",
    "        # Add the edge connecting the Track and the Country \n",
    "        g.add((Track, SO[\"isAvailableIn\"], Country))  \n",
    "\n",
    "\n",
    "    # Load artists as array\n",
    "    artists = row[\"artists\"].split(\",\")\n",
    "\n",
    "    for artist in artists:\n",
    "        # Create the RDF node\n",
    "        Artist = URIRef(SO[artist])\n",
    "\n",
    "        # Add the edge connecting the Track and the Artist\n",
    "        g.add((Artist, SO[\"partecipateIn\"], Track))  \n",
    "\n",
    "    \n",
    "    #Retrieve albumID\n",
    "    albumID = row[\"album\"]\n",
    "\n",
    "    # Create the RDF node\n",
    "    Album = URIRef(SO[albumID])\n",
    "\n",
    "    # Add the edge connecting the Track and the Artist\n",
    "    g.add((Track, SO[\"isPartOf\"], Album))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ðŸ’¾] SAVING\n"
     ]
    }
   ],
   "source": [
    "# Save all the data in the Turtle format\n",
    "print(\"[ðŸ’¾] SAVING\")\n",
    "with open(tracksTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(g.serialize(format=\"turtle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Graph\n",
    "g = createGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "charts = pd.read_csv(spotifyChartsPath , sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the charts DataFrame\n",
    "\n",
    "\"\"\"\n",
    "A  differenza degli altri csv dove ho un id ben preciso, qui devo creare un id che rappresenti quella specifica classifica \n",
    "combinando il paese di riferimento e la data di riferimento in modo da ottenere degli id univoci.\n",
    "\n",
    "Anche per il nome della classifica penso vada fatto un discorso analogo, mentre per il numero di tracce e per la tipologia\n",
    "possiamo usare dei valori fissi visto che abbiamo solamente questi dati\n",
    "\n",
    "Il problema Ã¨ che dallo stesso file dobbiamo ottenere sia gli id per costruire gli oggetti di tipo classifica che i dati \n",
    "sia i dati necessari a collegare una specifica track a una posizione in classifica\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" \n",
    "for row in tracks.iterrows():\n",
    "    #Retrieve info about the chart\n",
    "    country = row[\"country\"]\n",
    "    date = row[\"date\"]\n",
    "\n",
    "    #Create a uniqueID\n",
    "    chartID = \"TOP100_\" + country + \"_\" + date\n",
    "\n",
    "    # Create the node to add to the Graph \n",
    "    Chart = URIRef(SO[chartID])\n",
    "\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Chart, RDF.type, SO.Chart))\n",
    "\n",
    "    # Add the name of the Chart\n",
    "    g.add((Track, SO[\"name\"], Literal(\"TOP 100 \" + Country, datatype=XSD.string)))\n",
    "\n",
    "    # Add the  date of the chart\n",
    "    g.add((Album, SO[\"date\"], Literal(date, datatype=XSD.date)))    \n",
    "    \n",
    "    # Add the number of tracks\n",
    "    g.add((Album, SO[\"totalTracks\"], Literal(\"100\", datatype=XSD.int)))\n",
    "\n",
    "    # Add chart type\n",
    "    chartType = URIRef(SO[\"Top\"]])\n",
    "    g.add((Chart, SO[\"isTypeOf\"], chartType))  \n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "22d38fa0cb949c4d83b127034afdff90d77a7338f5681221558c482c7c131893"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
