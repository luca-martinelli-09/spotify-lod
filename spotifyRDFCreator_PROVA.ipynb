{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "# Load the required libraries\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace\n",
    "\n",
    "# rdflib knows about some namespaces, like FOAF\n",
    "from rdflib.namespace import FOAF, XSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get absolute path\n",
    "absPath = str(Path(os.path.abspath(os.getcwd())).absolute())\n",
    "datasetsPath = os.path.join(absPath, \"datasets\")\n",
    "rdfPath = os.path.join(absPath, \"rdf\")\n",
    "\n",
    "# Create dataset directory if not exists\n",
    "if not os.path.exists(datasetsPath):\n",
    "    os.mkdir(datasetsPath)\n",
    "\n",
    "# Create RDF directory if not exists\n",
    "if not os.path.exists(rdfPath):\n",
    "    os.mkdir(rdfPath)\n",
    "\n",
    "# Setup datasets paths\n",
    "spotifyChartsPath = os.path.join(datasetsPath, \"reducedSpotifyCharts.csv\")\n",
    "genresPath = os.path.join(datasetsPath, \"genres.csv\")\n",
    "marketsPath = os.path.join(datasetsPath, \"markets.csv\")\n",
    "tracksPath = os.path.join(datasetsPath, \"tracks.csv\")\n",
    "albumsPath = os.path.join(datasetsPath, \"albums.csv\")\n",
    "artistsPath = os.path.join(datasetsPath, \"artists.csv\")\n",
    "peoplePath = os.path.join(datasetsPath, \"people.csv\")\n",
    "\n",
    "countriesPath = os.path.join(datasetsPath, \"countries2.csv\")\n",
    "altCountriesPath = os.path.join(datasetsPath, \"altCountries.csv\")\n",
    "\n",
    "# Setup Turtle paths\n",
    "genresTTLPath = os.path.join(rdfPath, \"genres.ttl\")\n",
    "marketsTTLPath = os.path.join(rdfPath, \"markets.ttl\")\n",
    "tracksTTLPath = os.path.join(rdfPath, \"tracks.ttl\")\n",
    "albumsTTLPath = os.path.join(rdfPath, \"albums.ttl\")\n",
    "artistsTTLPath = os.path.join(rdfPath, \"artists.ttl\")\n",
    "peopleTTLPath = os.path.join(rdfPath, \"people.ttl\")\n",
    "chartsTTLPath = os.path.join(rdfPath, \"charts.ttl\")\n",
    "appearanceTTLPath = os.path.join(rdfPath, \"appearance.ttl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the ontologies namespaces not known by RDFlib\n",
    "\n",
    "# Country Ontology\n",
    "CNS = Namespace(\"http://eulersharp.sourceforge.net/2003/03swap/countries#\")\n",
    "\n",
    "# Spotify Ontology\n",
    "SO = Namespace(\"https://www.dei.unipd.it/~martinelli/spotify/spotifyOntology#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createGraph():\n",
    "    # Create the graph\n",
    "    g = Graph()\n",
    "\n",
    "    # Bind the namespaces to a prefix for more readable output\n",
    "    g.bind(\"foaf\", FOAF)\n",
    "    g.bind(\"xsd\", XSD)\n",
    "    g.bind(\"countries\", CNS)\n",
    "    g.bind(\"so\", SO)\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Graph\n",
    "g = createGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "genres = pd.read_csv(genresPath, sep=\",\", index_col=\"genre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createGenreID(genre):\n",
    "    # Replace all special chars with \"-\"\n",
    "    genreID = \"\"\n",
    "    for char in genre:\n",
    "        genreID += char if char.isalnum() else \"-\"\n",
    "        \n",
    "    return genreID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the album DataFrame\n",
    "for genre, row in genres.iterrows():\n",
    "    # Create genre ID from name\n",
    "    genreID = createGenreID(genre)\n",
    "\n",
    "    # Create the node to add to the Graph\n",
    "    Genre = URIRef(SO[genreID])\n",
    "\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Genre, RDF.type, SO.Genre))\n",
    "\n",
    "    # Add the name of the genre\n",
    "    g.add((Genre, SO[\"name\"], Literal(genre, datatype=XSD.string)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the data in the Turtle format\n",
    "print(\"[ðŸ’¾] SAVING\")\n",
    "with open(genresTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(g.serialize(format=\"turtle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Graph\n",
    "g = createGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "artists = pd.read_csv(artistsPath, sep=\",\", index_col=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the album DataFrame\n",
    "for artistID, row in artists.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    Artist = URIRef(SO[artistID])\n",
    "\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Artist, RDF.type, SO.Artist))\n",
    "\n",
    "    # Add the name of the artist\n",
    "    g.add((Artist, SO[\"name\"], Literal(row[\"name\"], datatype=XSD.string)))\n",
    "\n",
    "    # Add the popularity of the artist\n",
    "    g.add((Artist, SO[\"popularity\"], Literal(row[\"popularity\"], datatype=XSD.int)))\n",
    "\n",
    "    # Load genres as array\n",
    "    genres = row[\"genres\"].split(\",\") if not pd.isnull(row[\"genres\"]) else []\n",
    "\n",
    "    for genre in genres:\n",
    "        # Create the RDF node\n",
    "        Genre = URIRef(SO[createGenreID(genre)])\n",
    "\n",
    "        # Add the edge connecting the Album and the Country\n",
    "        g.add((Artist, SO[\"hasGenre\"], Genre))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the data in the Turtle format\n",
    "print(\"[ðŸ’¾] SAVING\")\n",
    "with open(artistsTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(g.serialize(format=\"turtle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Albums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Graph\n",
    "g = createGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "albums = pd.read_csv(albumsPath, sep=\",\", index_col=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the album DataFrame\n",
    "for albumID, row in albums.iterrows():\n",
    "    # Create the node to add to the Graph \n",
    "    Album = URIRef(SO[albumID])\n",
    "\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Album, RDF.type, SO.Album))\n",
    "\n",
    "    # Add the name of the album\n",
    "    g.add((Album, SO[\"name\"], Literal(row[\"title\"], datatype=XSD.string)))\n",
    "    \n",
    "    # Add the total tracks of the album\n",
    "    g.add((Album, SO[\"totalTracks\"], Literal(row[\"total_tracks\"], datatype=XSD.int)))\n",
    "\n",
    "    # Manage release date taking into account release precision\n",
    "    releaseDate = row[\"release_date\"]\n",
    "    if(row[\"release_date_precision\"]==\"year\"):\n",
    "        releaseDate += \"-01-01\"\n",
    "    elif(row[\"release_date_precision\"]==\"month\"):\n",
    "        releaseDate += \"-01\"\n",
    "    \n",
    "    # Add the release date of the album\n",
    "    g.add((Album, SO[\"releaseDate\"], Literal(releaseDate, datatype=XSD.date)))    \n",
    "    \n",
    "    # Add album type\n",
    "    albumType = URIRef(SO[row[\"album_type\"]])\n",
    "    g.add((Album, SO[\"isTypeOf\"], albumType))  \n",
    "\n",
    "    # Load countries as array\n",
    "    countries = row[\"available_countries\"].split(\",\") if not pd.isnull(row[\"available_countries\"]) else []\n",
    "\n",
    "    for country in countries:\n",
    "        # Create the RDF node\n",
    "        Country = URIRef(CNS[country.lower()])\n",
    "\n",
    "        # Add the edge connecting the Album and the Country \n",
    "        g.add((Album, SO[\"isAvailableIn\"], Country))  \n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the data in the Turtle format\n",
    "print(\"[ðŸ’¾] SAVING\")\n",
    "with open(albumsTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(g.serialize(format=\"turtle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Graph\n",
    "g = createGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "tracks = pd.read_csv(tracksPath, sep=\",\", index_col=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the tracks DataFrame\n",
    "\n",
    "for trackID, row in tracks.iterrows():\n",
    "    # Create the node to add to the Graph \n",
    "    Track = URIRef(SO[trackID])\n",
    "\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Track, RDF.type, SO.Track))\n",
    "\n",
    "    # Add the name of the track\n",
    "    g.add((Track, SO[\"name\"], Literal(row[\"title\"], datatype=XSD.string)))\n",
    "    \n",
    "    # Add all the technical charateristics\n",
    "    g.add((Track, SO[\"duration\"], Literal(row[\"duration\"], datatype=XSD.int)))\n",
    "    g.add((Track, SO[\"popularity\"], Literal(row[\"popularity\"], datatype=XSD.int)))\n",
    "    g.add((Track, SO[\"explicit\"], Literal(row[\"explicit\"], datatype=XSD.boolean)))\n",
    "    g.add((Track, SO[\"key\"], Literal(row[\"key\"], datatype=XSD.int)))\n",
    "    g.add((Track, SO[\"tempo\"], Literal(row[\"tempo\"], datatype=XSD.float)))\n",
    "    g.add((Track, SO[\"mode\"], Literal(row[\"mode\"], datatype=XSD.int)))\n",
    "    g.add((Track, SO[\"time_signature\"], Literal(row[\"time_signature\"], datatype=XSD.int)))\n",
    "    g.add((Track, SO[\"acousticness\"], Literal(row[\"acousticness\"], datatype=XSD.float)))\n",
    "    g.add((Track, SO[\"danceability\"], Literal(row[\"danceability\"], datatype=XSD.float)))\n",
    "    g.add((Track, SO[\"energy\"], Literal(row[\"energy\"], datatype=XSD.float)))\n",
    "    g.add((Track, SO[\"loudness\"], Literal(row[\"loudness\"], datatype=XSD.float)))\n",
    "    g.add((Track, SO[\"liveness\"], Literal(row[\"liveness\"], datatype=XSD.float)))\n",
    "    g.add((Track, SO[\"valence\"], Literal(row[\"valence\"], datatype=XSD.float)))\n",
    "    g.add((Track, SO[\"speechiness\"], Literal(row[\"speechiness\"], datatype=XSD.float)))\n",
    "    g.add((Track, SO[\"instrumentalness\"], Literal(row[\"instrumentalness\"], datatype=XSD.float)))\n",
    "\n",
    "    # Load countries as array\n",
    "    countries = row[\"available_countries\"].split(\",\") if not pd.isnull(row[\"available_countries\"]) else []\n",
    "\n",
    "    for country in countries:\n",
    "        # Create the RDF node\n",
    "        Country = URIRef(CNS[country.lower()])\n",
    "\n",
    "        # Add the edge connecting the Track and the Country \n",
    "        g.add((Track, SO[\"isAvailableIn\"], Country))  \n",
    "\n",
    "\n",
    "    # Load artists as array\n",
    "    artists = row[\"artists\"].split(\",\")\n",
    "\n",
    "    for artist in artists:\n",
    "        # Create the RDF node\n",
    "        Artist = URIRef(SO[artist])\n",
    "\n",
    "        # Add the edge connecting the Track and the Artist\n",
    "        g.add((Artist, SO[\"partecipateIn\"], Track))  \n",
    "\n",
    "    \n",
    "    #Retrieve albumID\n",
    "    albumID = row[\"album\"]\n",
    "\n",
    "    # Create the RDF node\n",
    "    Album = URIRef(SO[albumID])\n",
    "\n",
    "    # Add the edge connecting the Track and the Artist\n",
    "    g.add((Track, SO[\"isPartOf\"], Album))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the data in the Turtle format\n",
    "print(\"[ðŸ’¾] SAVING\")\n",
    "with open(tracksTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(g.serialize(format=\"turtle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Graph\n",
    "g = createGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "charts = pd.read_csv(spotifyChartsPath , sep=\",\")\n",
    "\n",
    "#Loads other CSV\n",
    "countries = pd.read_csv(countriesPath , sep=\",\")\n",
    "countries.rename(columns={'Name': 'ISO_name'},inplace = True),\n",
    "\n",
    "altCountries = pd.read_csv(altCountriesPath , sep=\",\")\n",
    "altCountries.columns =['alternative_name','ISO_name']\n",
    "\n",
    "#Aggregate the original dataframe to identify a specific chart using COUNTRY and DATE\n",
    "chartsDF = charts.groupby(['country', 'date']).size().reset_index(name='total_tracks')\n",
    "\n",
    "#Removing global\n",
    "chartsDF.drop(index=chartsDF[chartsDF['country'] == 'Global'].index, inplace=True)\n",
    "charts.drop(index=chartsDF[chartsDF['country'] == 'Global'].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I iterate through the dataframe\n",
    "for index, row in chartsDF.iterrows():\n",
    "    \n",
    "    #Retrieve country and date\n",
    "    country_name = row[\"country\"]\n",
    "    date = row[\"date\"]\n",
    "\n",
    "    #Reformat date \n",
    "    date = datetime.datetime.strptime(date, '%d/%m/%Y').strftime('%d-%m-%y')\n",
    "\n",
    "    #Try to retrieve ISO CODE of the country\n",
    "    try: \n",
    "        matchedCountries = countries[countries['ISO_name'].str.contains(country_name)]\n",
    "        iso_country_code = matchedCountries['Code'].iloc[0]\n",
    "    except IndexError as e:\n",
    "        #Look is an alternative name was used\n",
    "        #retrieve ISO NAME\n",
    "        matchedCountries = altCountries[altCountries['alternative_name']==country_name]\n",
    "        iso_country_name = matchedCountries['ISO_name'].iloc[0]\n",
    "\n",
    "        #Retrieve ISO CODE\n",
    "        matchedCountries = countries[countries['ISO_name'].str.contains(iso_country_name)]\n",
    "        iso_country_code = matchedCountries['Code'].iloc[0]\n",
    "     \n",
    "    #Create a uniqueID \n",
    "    chartID = \"top-100-\" + iso_country_code + \"-\" + date\n",
    "\n",
    "    # Create the node to add to the Graph \n",
    "    Chart = URIRef(SO[chartID])\n",
    "\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Chart, RDF.type, SO.Chart))\n",
    "\n",
    "    # Add the name of the Chart\n",
    "    g.add((Chart, SO[\"name\"], Literal(\"TOP \" + str(row['total_tracks']) + \" \" + country_name, datatype=XSD.string)))\n",
    "\n",
    "    # Add the date of the chart\n",
    "    g.add((Chart, SO[\"date\"], Literal(date, datatype=XSD.date)))    \n",
    "    \n",
    "    # Add the number of tracks\n",
    "    g.add((Chart, SO[\"totalTracks\"], Literal(row['total_tracks'], datatype=XSD.int)))\n",
    "\n",
    "    # Add related Country\n",
    "    # Create the RDF node\n",
    "    Country = URIRef(CNS[iso_country_code.lower()])\n",
    "    # Add the edge connecting the Chart and the Country \n",
    "    g.add((Chart, SO[\"isReferredTo\"], Country))  \n",
    "\n",
    "    # Add chart type\n",
    "    chartType = URIRef(SO[\"Top\"])\n",
    "    g.add((Chart, SO[\"isTypeOf\"], chartType))  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the data in the Turtle format\n",
    "print(\"[ðŸ’¾] SAVING\")\n",
    "with open(chartsTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(g.serialize(format=\"turtle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Graph\n",
    "g = createGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I iterate through the dataframe\n",
    "for index, row in charts.iterrows():\n",
    "    \n",
    "    #Create a uniqueID \n",
    "    appeareanceID = \"appereance\"+str(index)\n",
    "\n",
    "    # Create the node to add to the Graph \n",
    "    Appereance = URIRef(SO[appeareanceID])\n",
    "\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Appereance, RDF.type, SO.Appereance))\n",
    "\n",
    "    # Add the position of track\n",
    "    g.add((Chart, SO[\"position\"], Literal(row['position'], datatype=XSD.int)))\n",
    "\n",
    "    \n",
    "    # Get the track id from the uri\n",
    "    trackID = row['uri'].removeprefix(\"https://open.spotify.com/track/\")\n",
    "\n",
    "\n",
    "    # Add the edge connecting Appereance to the Track\n",
    "    Track = URIRef(SO[trackID])\n",
    "    g.add((Track, SO[\"appearsIn\"], Track))  \n",
    "\n",
    "\n",
    "\n",
    "    \"\"\" *******************************************************************************************************\"\"\"\n",
    "    \"\"\" TUTTA QUESTA PARTE Ã¨ UGALE A PRIMA QUINDI FORSE BASTA FARE UNA FUNZIONE TIPO PER RICAVRE ISO CODE O PER GENERARE ID DELLA CHART\"\"\"\n",
    "    #Compose the Chart ID using data from CSV\n",
    "    country_name = row[\"country\"]\n",
    "    date = row[\"date\"]\n",
    "    date = datetime.datetime.strptime(date, '%d/%m/%Y').strftime('%d-%m-%y')\n",
    "\n",
    "    #Try to retrieve ISO CODE of the country\n",
    "    try: \n",
    "        matchedCountries = countries[countries['ISO_name'].str.contains(country_name)]\n",
    "        iso_country_code = matchedCountries['Code'].iloc[0]\n",
    "    except IndexError as e:\n",
    "        #Look is an alternative name was used\n",
    "        #retrieve ISO NAME\n",
    "        matchedCountries = altCountries[altCountries['alternative_name']==country_name]\n",
    "        iso_country_name = matchedCountries['ISO_name'].iloc[0]\n",
    "\n",
    "        #Retrieve ISO CODE\n",
    "        matchedCountries = countries[countries['ISO_name'].str.contains(iso_country_name)]\n",
    "        iso_country_code = matchedCountries['Code'].iloc[0]\n",
    "    \n",
    "    chartID = \"top-100-\" + iso_country_code + \"-\" + date\n",
    "    \"\"\" ************************************************************************************ \"\"\"\n",
    "\n",
    "    # Create the node to add to the Graph \n",
    "    Chart = URIRef(SO[chartID])\n",
    "\n",
    "    g.add((Appereance, SO[\"isPositionedIn\"], Chart))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the data in the Turtle format\n",
    "print(\"[ðŸ’¾] SAVING\")\n",
    "with open(appearanceTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(g.serialize(format=\"turtle\"))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "22d38fa0cb949c4d83b127034afdff90d77a7338f5681221558c482c7c131893"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
