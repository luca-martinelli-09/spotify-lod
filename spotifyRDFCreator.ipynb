{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "# Load the required libraries\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace\n",
    "\n",
    "# rdflib knows about some namespaces, like FOAF\n",
    "from rdflib.namespace import FOAF, XSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get absolute path\n",
    "absPath = str(Path(os.path.abspath(os.getcwd())).absolute())\n",
    "datasetsPath = os.path.join(absPath, \"datasets\")\n",
    "rdfPath = os.path.join(absPath, \"rdf\")\n",
    "\n",
    "# Create dataset directory if not exists\n",
    "if not os.path.exists(datasetsPath):\n",
    "    os.mkdir(datasetsPath)\n",
    "\n",
    "# Create RDF directory if not exists\n",
    "if not os.path.exists(rdfPath):\n",
    "    os.mkdir(rdfPath)\n",
    "\n",
    "# Setup datasets paths\n",
    "spotifyChartsPath = os.path.join(datasetsPath, \"reducedSpotifyCharts.csv\")\n",
    "genresPath = os.path.join(datasetsPath, \"genres.csv\")\n",
    "marketsPath = os.path.join(datasetsPath, \"markets.csv\")\n",
    "tracksPath = os.path.join(datasetsPath, \"tracks.csv\")\n",
    "albumsPath = os.path.join(datasetsPath, \"albums.csv\")\n",
    "artistsPath = os.path.join(datasetsPath, \"artists.csv\")\n",
    "peoplePath = os.path.join(datasetsPath, \"people.csv\")\n",
    "\n",
    "# Countries\n",
    "countriesPath = os.path.join(datasetsPath, \"countries2.csv\")\n",
    "altCountriesPath = os.path.join(datasetsPath, \"altCountries.csv\")\n",
    "\n",
    "# Setup Turtle paths\n",
    "genresTTLPath = os.path.join(rdfPath, \"genres.ttl\")\n",
    "marketsTTLPath = os.path.join(rdfPath, \"markets.ttl\")\n",
    "tracksTTLPath = os.path.join(rdfPath, \"tracks.ttl\")\n",
    "albumsTTLPath = os.path.join(rdfPath, \"albums.ttl\")\n",
    "artistsTTLPath = os.path.join(rdfPath, \"artists.ttl\")\n",
    "peopleTTLPath = os.path.join(rdfPath, \"people.ttl\")\n",
    "chartsTTLPath = os.path.join(rdfPath, \"charts.ttl\")\n",
    "appearanceTTLPath = os.path.join(rdfPath, \"appearance.ttl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the ontologies namespaces not known by RDFlib\n",
    "\n",
    "# Country Ontology\n",
    "CNS = Namespace(\"http://eulersharp.sourceforge.net/2003/03swap/countries#\")\n",
    "\n",
    "# Spotify Ontology\n",
    "SO = Namespace(\"https://www.dei.unipd.it/~martinelli/spotify/spotifyOntology#\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createGraph():\n",
    "    # Create the graph\n",
    "    g = Graph()\n",
    "\n",
    "    # Bind the namespaces to a prefix for more readable output\n",
    "    g.bind(\"foaf\", FOAF)\n",
    "    g.bind(\"xsd\", XSD)\n",
    "    g.bind(\"countries\", CNS)\n",
    "    g.bind(\"so\", SO)\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadCountries():\n",
    "    countries = pd.read_csv(countriesPath, sep=\",\")\n",
    "    altCountries = pd.read_csv(altCountriesPath, sep=\",\")\n",
    "    altCountries.columns = [\"AlternativeName\", \"Name\"]\n",
    "    \n",
    "    return countries, altCountries\n",
    "\n",
    "\n",
    "def getCountryCode(countryName, countries, altCountries):\n",
    "    # Try to retrieve ISO CODE of the country\n",
    "    try:\n",
    "        matchedCountries = countries[countries[\"Name\"].str.contains(countryName)]\n",
    "        countryCode = matchedCountries[\"Code\"].iloc[0]\n",
    "    except IndexError as e:\n",
    "        # Look if an alternative name was used\n",
    "        alternativeMatchedCountries = altCountries[altCountries[\"AlternativeName\"].str.contains(countryName)]\n",
    "        countryName = alternativeMatchedCountries[\"Name\"].iloc[0]\n",
    "\n",
    "        matchedCountries = countries[countries[\"Name\"] == countryName]\n",
    "        countryCode = matchedCountries[\"Code\"].iloc[0]\n",
    "    \n",
    "    return countryCode, countryName\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Graph\n",
    "g = createGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "genres = pd.read_csv(genresPath, sep=\",\", index_col=\"genre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createGenreID(genre):\n",
    "    # Replace all special chars with \"-\"\n",
    "    genreID = \"\"\n",
    "    for char in genre:\n",
    "        genreID += char if char.isalnum() else \"-\"\n",
    "        \n",
    "    return genreID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the album DataFrame\n",
    "for genre, row in genres.iterrows():\n",
    "    # Create genre ID from name\n",
    "    genreID = createGenreID(genre)\n",
    "\n",
    "    # Create the node to add to the Graph\n",
    "    Genre = URIRef(SO[genreID])\n",
    "\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Genre, RDF.type, SO.Genre))\n",
    "\n",
    "    # Add the name of the genre\n",
    "    g.add((Genre, SO[\"name\"], Literal(genre, datatype=XSD.string)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the data in the Turtle format\n",
    "print(\"[ðŸ’¾] SAVING\")\n",
    "with open(genresTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(g.serialize(format=\"turtle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Graph\n",
    "g = createGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "artists = pd.read_csv(artistsPath, sep=\",\", index_col=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the album DataFrame\n",
    "for artistID, row in artists.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    Artist = URIRef(SO[artistID])\n",
    "\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Artist, RDF.type, SO.Artist))\n",
    "\n",
    "    # Add the name of the artist\n",
    "    g.add((Artist, SO[\"name\"], Literal(row[\"name\"], datatype=XSD.string)))\n",
    "\n",
    "    # Add the popularity of the artist\n",
    "    g.add((Artist, SO[\"popularity\"], Literal(row[\"popularity\"], datatype=XSD.int)))\n",
    "\n",
    "    # Load genres as array\n",
    "    genres = row[\"genres\"].split(\",\") if not pd.isnull(row[\"genres\"]) else []\n",
    "\n",
    "    for genre in genres:\n",
    "        # Create the RDF node\n",
    "        Genre = URIRef(SO[createGenreID(genre)])\n",
    "\n",
    "        # Add the edge connecting the Album and the Country\n",
    "        g.add((Artist, SO[\"hasGenre\"], Genre))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the data in the Turtle format\n",
    "print(\"[ðŸ’¾] SAVING\")\n",
    "with open(artistsTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(g.serialize(format=\"turtle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Albums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Graph\n",
    "g = createGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "albums = pd.read_csv(albumsPath, sep=\",\", index_col=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the album DataFrame\n",
    "for albumID, row in albums.iterrows():\n",
    "    # Create the node to add to the Graph \n",
    "    Album = URIRef(SO[albumID])\n",
    "\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Album, RDF.type, SO.Album))\n",
    "\n",
    "    # Add the name of the album\n",
    "    g.add((Album, SO[\"name\"], Literal(row[\"title\"], datatype=XSD.string)))\n",
    "    \n",
    "    # Add the total tracks of the album\n",
    "    g.add((Album, SO[\"totalTracks\"], Literal(row[\"total_tracks\"], datatype=XSD.int)))\n",
    "\n",
    "    # Manage release date taking into account release precision\n",
    "    releaseDate = row[\"release_date\"]\n",
    "    if(row[\"release_date_precision\"]==\"year\"):\n",
    "        releaseDate += \"-01-01\"\n",
    "    elif(row[\"release_date_precision\"]==\"month\"):\n",
    "        releaseDate += \"-01\"\n",
    "    \n",
    "    # Add the release date of the album\n",
    "    g.add((Album, SO[\"releaseDate\"], Literal(releaseDate, datatype=XSD.date)))    \n",
    "    \n",
    "    # Add album type\n",
    "    albumType = URIRef(SO[row[\"album_type\"]])\n",
    "    g.add((Album, SO[\"isTypeOf\"], albumType))  \n",
    "\n",
    "    # Load countries as array\n",
    "    countries = row[\"available_countries\"].split(\",\") if not pd.isnull(row[\"available_countries\"]) else []\n",
    "\n",
    "    for country in countries:\n",
    "        # Create the RDF node\n",
    "        Country = URIRef(CNS[country.lower()])\n",
    "\n",
    "        # Add the edge connecting the Album and the Country \n",
    "        g.add((Album, SO[\"isAvailableIn\"], Country))\n",
    "    \n",
    "    # Load artists as array\n",
    "    artists = row[\"artists\"].split(\",\")\n",
    "\n",
    "    for artistID in artists:\n",
    "        # Create the RDF node\n",
    "        Artist = URIRef(SO[artistID])\n",
    "\n",
    "        # Add the edge connecting the Album and the Artist\n",
    "        g.add((Artist, SO[\"partecipateIn\"], Album))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the data in the Turtle format\n",
    "print(\"[ðŸ’¾] SAVING\")\n",
    "with open(albumsTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(g.serialize(format=\"turtle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Graph\n",
    "g = createGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "tracks = pd.read_csv(tracksPath, sep=\",\", index_col=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the tracks DataFrame\n",
    "\n",
    "for trackID, row in tracks.iterrows():\n",
    "    # Create the node to add to the Graph\n",
    "    Track = URIRef(SO[trackID])\n",
    "\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Track, RDF.type, SO.Track))\n",
    "\n",
    "    # Add the name of the track\n",
    "    g.add((Track, SO[\"name\"], Literal(row[\"title\"], datatype=XSD.string)))\n",
    "\n",
    "    # Add all the technical charateristics\n",
    "    g.add((Track, SO[\"duration\"], Literal(row[\"duration\"], datatype=XSD.int)))\n",
    "    g.add((Track, SO[\"popularity\"], Literal(row[\"popularity\"], datatype=XSD.int)))\n",
    "    g.add((Track, SO[\"explicit\"], Literal(row[\"explicit\"], datatype=XSD.boolean)))\n",
    "    g.add((Track, SO[\"key\"], Literal(row[\"key\"], datatype=XSD.int)))\n",
    "    g.add((Track, SO[\"tempo\"], Literal(row[\"tempo\"], datatype=XSD.float)))\n",
    "    g.add((Track, SO[\"mode\"], Literal(row[\"mode\"], datatype=XSD.int)))\n",
    "    g.add((Track, SO[\"time_signature\"], Literal(row[\"time_signature\"], datatype=XSD.int)))\n",
    "    g.add((Track, SO[\"acousticness\"], Literal(row[\"acousticness\"], datatype=XSD.float)))\n",
    "    g.add((Track, SO[\"danceability\"], Literal(row[\"danceability\"], datatype=XSD.float)))\n",
    "    g.add((Track, SO[\"energy\"], Literal(row[\"energy\"], datatype=XSD.float)))\n",
    "    g.add((Track, SO[\"loudness\"], Literal(row[\"loudness\"], datatype=XSD.float)))\n",
    "    g.add((Track, SO[\"liveness\"], Literal(row[\"liveness\"], datatype=XSD.float)))\n",
    "    g.add((Track, SO[\"valence\"], Literal(row[\"valence\"], datatype=XSD.float)))\n",
    "    g.add((Track, SO[\"speechiness\"], Literal(row[\"speechiness\"], datatype=XSD.float)))\n",
    "    g.add((Track, SO[\"instrumentalness\"], Literal(row[\"instrumentalness\"], datatype=XSD.float)))\n",
    "\n",
    "    # Load countries as array\n",
    "    countries = row[\"available_countries\"].split(\",\") if not pd.isnull(row[\"available_countries\"]) else []\n",
    "\n",
    "    for country in countries:\n",
    "        # Create the RDF node\n",
    "        Country = URIRef(CNS[country.lower()])\n",
    "\n",
    "        # Add the edge connecting the Track and the Country\n",
    "        g.add((Track, SO[\"isAvailableIn\"], Country))\n",
    "\n",
    "    # Load artists as array\n",
    "    artists = row[\"artists\"].split(\",\")\n",
    "\n",
    "    for artistID in artists:\n",
    "        # Create the RDF node\n",
    "        Artist = URIRef(SO[artistID])\n",
    "\n",
    "        # Add the edge connecting the Track and the Artist\n",
    "        g.add((Artist, SO[\"partecipateIn\"], Track))\n",
    "\n",
    "    #Retrieve albumID\n",
    "    albumID = row[\"album\"]\n",
    "\n",
    "    # Create the RDF node\n",
    "    Album = URIRef(SO[albumID])\n",
    "\n",
    "    # Add the edge connecting the Track and the Artist\n",
    "    g.add((Track, SO[\"isPartOf\"], Album))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the data in the Turtle format\n",
    "print(\"[ðŸ’¾] SAVING\")\n",
    "with open(tracksTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(g.serialize(format=\"turtle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Graph\n",
    "g = createGraph()\n",
    "\n",
    "# Load countries dataframe\n",
    "countries, altCountries = loadCountries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "charts = pd.read_csv(spotifyChartsPath , sep=\",\")\n",
    "\n",
    "# Aggregate the original dataframe to identify a specific chart using COUNTRY and DATE\n",
    "chartsDF = charts.groupby(\n",
    "    [\"country\", \"date\"]).size().reset_index(name=\"total_tracks\")\n",
    "\n",
    "# Removing global\n",
    "chartsDF = chartsDF.drop(index=chartsDF[chartsDF[\"country\"] == \"Global\"].index)\n",
    "charts = charts.drop(index=charts[charts[\"country\"] == \"Global\"].index)\n",
    "\n",
    "totalRows = len(charts.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I iterate through the dataframe\n",
    "for index, row in chartsDF.iterrows():\n",
    "\n",
    "    # Retrieve country and date\n",
    "    countryName = row[\"country\"]\n",
    "    date = row[\"date\"]\n",
    "    topNumType = 100\n",
    "    numTotalTracks = row[\"total_tracks\"]\n",
    "\n",
    "    # Reformat date\n",
    "    date = datetime.datetime.strptime(date, \"%d/%m/%Y\").strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Get the country code\n",
    "    countryCode, _ = getCountryCode(countryName, countries, altCountries)\n",
    "\n",
    "    # Create a uniqueID\n",
    "    chartID = \"top-{}-{}-{}\".format(topNumType, countryCode, date)\n",
    "\n",
    "    # Create the node to add to the Graph\n",
    "    Chart = URIRef(SO[chartID])\n",
    "\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Chart, RDF.type, SO.Chart))\n",
    "\n",
    "    # Add the name of the Chart\n",
    "    chartName = \"TOP {} {}\".format(topNumType, countryName)\n",
    "    g.add((Chart, SO[\"name\"], Literal(chartName, datatype=XSD.string)))\n",
    "\n",
    "    # Add the date of the chart\n",
    "    g.add((Chart, SO[\"date\"], Literal(date, datatype=XSD.date)))\n",
    "\n",
    "    # Add the number of tracks\n",
    "    g.add((Chart, SO[\"totalTracks\"], Literal(row['total_tracks'], datatype=XSD.int)))\n",
    "\n",
    "    # Add related Country\n",
    "    # Create the RDF node\n",
    "    Country = URIRef(CNS[countryCode.lower()])\n",
    "   \n",
    "    # Add the edge connecting the Chart and the Country\n",
    "    g.add((Chart, SO[\"isReferredTo\"], Country))\n",
    "\n",
    "    # Add chart type\n",
    "    chartType = URIRef(SO[\"top\"])\n",
    "    g.add((Chart, SO[\"isTypeOf\"], chartType))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the data in the Turtle format\n",
    "print(\"[ðŸ’¾] SAVING\")\n",
    "with open(chartsTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(g.serialize(format=\"turtle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appearance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Graph\n",
    "g = createGraph()\n",
    "\n",
    "# Load countries dataframe\n",
    "countries, altCountries = loadCountries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I iterate through the dataframe\n",
    "realIndex = 0\n",
    "\n",
    "for index, row in charts.iterrows():\n",
    "\n",
    "    # Create a uniqueID\n",
    "    appearanceID = \"appearance-{}\".format(index)\n",
    "\n",
    "    # Create the node to add to the Graph\n",
    "    Appearance = URIRef(SO[appearanceID])\n",
    "\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Appearance, RDF.type, SO.Appearance))\n",
    "\n",
    "    # Add the position of track\n",
    "    g.add((Chart, SO[\"position\"], Literal(row['position'], datatype=XSD.int)))\n",
    "\n",
    "    # Get the track id from the uri\n",
    "    trackID = row['uri'].removeprefix(\"https://open.spotify.com/track/\")\n",
    "\n",
    "    # Add the edge connecting Appearance to the Track\n",
    "    Track = URIRef(SO[trackID])\n",
    "    g.add((Track, SO[\"appearsIn\"], Appearance))\n",
    "\n",
    "    # Retrieve country and date\n",
    "    countryName = row[\"country\"]\n",
    "    date = row[\"date\"]\n",
    "\n",
    "    # Reformat date\n",
    "    date = datetime.datetime.strptime(date, \"%d/%m/%Y\").strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Get the country code\n",
    "    countryCode, _ = getCountryCode(countryName, countries, altCountries)\n",
    "\n",
    "    # Create a uniqueID\n",
    "    chartID = \"top-100-{}-{}\".format(countryCode, date)\n",
    "\n",
    "    # Create the node to add to the Graph\n",
    "    Chart = URIRef(SO[chartID])\n",
    "\n",
    "    g.add((Appearance, SO[\"isPositionedIn\"], Chart))\n",
    "\n",
    "    realIndex += 1\n",
    "\n",
    "    if realIndex % 10000 == 0:\n",
    "        print(\"ðŸ’¾ [STATUS INFO] {row}/{totalRows} ({percentage:.2f}%)\\n\".format(\n",
    "            row=realIndex, totalRows=totalRows, percentage=((realIndex * 100) / totalRows)))\n",
    "\n",
    "print(\"ðŸ’¾ [STATUS INFO] {row}/{totalRows} ({percentage:.2f}%)\\n\".format(\n",
    "    row=realIndex, totalRows=totalRows, percentage=((realIndex * 100) / totalRows)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the data in the Turtle format\n",
    "print(\"[ðŸ’¾] SAVING\")\n",
    "with open(appearanceTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(g.serialize(format=\"turtle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# People"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Graph\n",
    "g = createGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "people = pd.read_csv(peoplePath, sep=\",\", index_col=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoCompleteDate(dateStr):\n",
    "    if dateStr is not None:\n",
    "        if len(dateStr) == 4:\n",
    "            dateStr += \"-01-01\"\n",
    "        elif len(dateStr) == 7:\n",
    "            dateStr += \"-01\"\n",
    "\n",
    "    return dateStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedPeople = people.groupby([\"id\"])\n",
    "\n",
    "index = 0\n",
    "\n",
    "# iterate over each group\n",
    "for peopleID, peopleGroup in groupedPeople:\n",
    "\n",
    "    name = peopleGroup.iloc[0][\"name\"] if not pd.isnull(\n",
    "        peopleGroup.iloc[0][\"name\"]) else \"\"\n",
    "    surname = peopleGroup.iloc[0][\"surname\"] if not pd.isnull(\n",
    "        peopleGroup.iloc[0][\"surname\"]) else \"\"\n",
    "    birthDate= peopleGroup.iloc[0][\"birthdate\"] if not pd.isnull(\n",
    "        peopleGroup.iloc[0][\"birthdate\"]) else \"\"\n",
    "    deathDate= peopleGroup.iloc[0][\"deathdate\"] if not pd.isnull(\n",
    "        peopleGroup.iloc[0][\"deathdate\"]) else \"\"\n",
    "    nationality= peopleGroup.iloc[0][\"nationality\"] if not pd.isnull(\n",
    "        peopleGroup.iloc[0][\"nationality\"]) else \"\"\n",
    "\n",
    "    # Get the list of artists in which the person appear\n",
    "    artists = []\n",
    "    for _, row in peopleGroup.iterrows():\n",
    "        artists.append(row[\"artist\"])\n",
    "\n",
    "    # Create a uniqueID\n",
    "    peopleID = \"people-{}\".format(index)\n",
    "    index += 1\n",
    "\n",
    "    # Create the node to add to the Graph\n",
    "    People = URIRef(SO[peopleID])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((People, RDF.type, SO.People))\n",
    "\n",
    "    # Add name and surname\n",
    "    g.add((People, SO[\"name\"], Literal(name, datatype=XSD.string)))\n",
    "    g.add((People, SO[\"surname\"], Literal(\n",
    "        surname, datatype=XSD.string)))\n",
    "\n",
    "    # Manage dates\n",
    "    birthDate = autoCompleteDate(birthDate)\n",
    "    birthDate = birthDate if not birthDate is None else \"\"\n",
    "    \n",
    "    deathDate = autoCompleteDate(deathDate)\n",
    "    deathDate = deathDate if not deathDate is None else \"\"\n",
    "\n",
    "    g.add((People, SO[\"birthdate\"], Literal(birthDate, datatype=XSD.date)))\n",
    "    g.add((People, SO[\"deathdate\"], Literal(deathDate, datatype=XSD.date)))\n",
    "\n",
    "    # Add nationality\n",
    "    # Create the RDF node\n",
    "    if nationality is not None:\n",
    "        Country = URIRef(CNS[nationality.lower()])\n",
    "        # Add the edge connecting People and the Country\n",
    "        g.add((People, SO[\"hasNationality\"], Country))\n",
    "\n",
    "    #manage edge connectig artists and people\n",
    "    for artistID in artists:\n",
    "        # Create the RDF node\n",
    "        Artist = URIRef(SO[artistID])\n",
    "\n",
    "        # Add the edge connecting the Track and the Artist\n",
    "        g.add((People, SO[\"isMemberOf\"], Artist))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the data in the Turtle format\n",
    "print(\"[ðŸ’¾] SAVING\")\n",
    "with open(peopleTTLPath, \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(g.serialize(format=\"turtle\"))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "22d38fa0cb949c4d83b127034afdff90d77a7338f5681221558c482c7c131893"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
